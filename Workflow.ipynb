{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43326be6-68a8-45e6-b0f8-9c0ee0c67a58",
   "metadata": {},
   "source": [
    "# Complete Workflow testing\n",
    "This notebook processes the full workflow from raw data to trained model for future usage with the preferred MLOps Tool Stack.\n",
    "In this case the relevant components will be DVC for Data Versioning, MLflow for Experiment Tracking and Model Registry and Prefect for Workflow Orchestration.\n",
    "Everything in this notebook is adapted to the specific customer segmentation project of a small car repair shop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f11f1-4b6c-4892-9993-33bad6a263d2",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ed8723-6c0c-46b0-8648-3893f70ab897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utils\n",
    "ERROR_MESSAGE_OSERROR = \"File '{0}' not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816d127-9f20-4add-9ee0-17213c7c2dd0",
   "metadata": {},
   "source": [
    "## Extract\n",
    "The extraction phase consists of\n",
    "- merging the raw text files\n",
    "- converting the text files to one single csv file\n",
    "- converting the csv file to a pandas DataFrame\n",
    "- processing the data (header name conversion, deleting unnessecary columns, normalizing, etc.)\n",
    "- converting the final DataFrame to a parquet file\n",
    "\n",
    "All steps are logged as MLflow runs with the relevant metadata and artifacts.\n",
    "Every step will be represented as a python function to easily create the corresponding python scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa3d00-4f6d-4c61-9326-a6f5846fe672",
   "metadata": {},
   "source": [
    "### Define common variables\n",
    "These variables will be parameters for the final python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ece6d21-7a2e-4db4-b25c-ce1d6cc3b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed for extraction\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f09bbf-9aa5-4c53-b0f0-bd8924494a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = 'data'\n",
    "tmp_file_path = '/tmp'\n",
    "raw_data_file_extensions = ['.TXT']\n",
    "raw_data_merged_file_name = 'data_merged.TXT'\n",
    "raw_data_encoding = 'iso8859_15'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa9fe2-6832-4d12-861b-ba61e86295d5",
   "metadata": {},
   "source": [
    "### Merging the raw text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888544af-9b33-441c-9574-f1dafbb1889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_raw_data(data_dir: str, out_file_path: str, *, data_dir_file_extensions: list, encoding='utf_8') -> str:\n",
    "    \"\"\"\n",
    "    Merge contents of multiple files into one output file.\n",
    "\n",
    "    Parameters:\n",
    "        data_dir (str): Path to the raw input files.\n",
    "        out_file_path (str): Full path to the output file (e.g. '/tmp/out.txt').\n",
    "        encoding (str, optional): Encoding of the input files (default 'utf_8').\n",
    "        \n",
    "    Returns: \n",
    "        (str): Full path of the output file.\n",
    "    \"\"\"\n",
    "    # Full relative path for each raw file\n",
    "    raw_files = sorted([os.path.join(raw_data_path, filename) for filename in os.listdir(data_dir)])\n",
    "    print(\"Raw files after path extensions:\")\n",
    "    print(raw_files)\n",
    "    \n",
    "    # Filter directories (in our case only files are necessary and allowed)\n",
    "    raw_files_dir_filter = [file_path for file_path in raw_files if os.path.isfile(file_path)]\n",
    "    if len(raw_files) != len(raw_files_dir_filter):\n",
    "        print(\"Raw files after filtering directories:\")\n",
    "        print(raw_files_dir_filter)\n",
    "    else:\n",
    "        print(\"No directories had to be filtered from raw files\")\n",
    "    raw_files = raw_files_dir_filter\n",
    "    \n",
    "    # If specific conditions where provided (data_dir_file_extensions), filter the raw_files list to only keep the permitted files\n",
    "    if data_dir_file_extensions is not None and len(data_dir_file_extensions) > 0:\n",
    "        raw_files_extension_filter = [filename for filename in raw_files if filename.endswith(tuple(data_dir_file_extensions))]\n",
    "        if len(raw_files_extension_filter) != len(raw_files_dir_filter):\n",
    "            print(\"Raw files after filtering by allowed extensions:\")\n",
    "            print(raw_files_extension_filter)\n",
    "        else:\n",
    "            print(\"No files had to be filtered by forbidden extension\")\n",
    "        raw_files = raw_files_extension_filter\n",
    "    else:\n",
    "        print(\"No allowed extensions where provided\")\n",
    "\n",
    "    # Perform merging\n",
    "    if len(raw_files) == 0:\n",
    "        raise Exception(f\"No raw_files left after filtering\")\n",
    "    with open(out_file_path, 'w', encoding=encoding) as merged:\n",
    "        for idx, file in enumerate(raw_files):\n",
    "            with open(file, 'r', encoding=encoding) as current_raw_file:\n",
    "                # The header information is only needed ones\n",
    "                if idx == 0:\n",
    "                    merged.writelines(current_raw_file.readlines())\n",
    "                else:\n",
    "                    merged.writelines(current_raw_file.readlines()[1:])\n",
    "                print(f\"Finished processing file {file}\")\n",
    "        \n",
    "    print(f\"Files merged in {out_file_path}\")\n",
    "    return out_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3447ce-afd2-4f2b-a788-0cc051667cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw files after path extensions:\n",
      "['data/.ipynb_checkpoints', 'data/2010_01_01-2013_12_31.TXT', 'data/2014_01_01-2015_12_31.TXT', 'data/2016_01_01-2017_12_31.TXT', 'data/2018_01_01-2019_12_31.TXT', 'data/2020_01_01-2021_12_31.TXT', 'data/2022_01_01-2024_08_06.TXT']\n",
      "Raw files after filtering directories:\n",
      "['data/2010_01_01-2013_12_31.TXT', 'data/2014_01_01-2015_12_31.TXT', 'data/2016_01_01-2017_12_31.TXT', 'data/2018_01_01-2019_12_31.TXT', 'data/2020_01_01-2021_12_31.TXT', 'data/2022_01_01-2024_08_06.TXT']\n",
      "No files had to be filtered by forbidden extension\n",
      "Finished processing file data/2010_01_01-2013_12_31.TXT\n",
      "Finished processing file data/2014_01_01-2015_12_31.TXT\n",
      "Finished processing file data/2016_01_01-2017_12_31.TXT\n",
      "Finished processing file data/2018_01_01-2019_12_31.TXT\n",
      "Finished processing file data/2020_01_01-2021_12_31.TXT\n",
      "Finished processing file data/2022_01_01-2024_08_06.TXT\n",
      "Files merged in /tmp/data_merged.TXT\n",
      "Execution of merging took: 0.8726873397827148 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data_merged_file_path = merge_raw_data(\n",
    "    raw_data_path, \n",
    "    os.path.join(tmp_file_path, raw_data_merged_file_name), \n",
    "    data_dir_file_extensions=raw_data_file_extensions, \n",
    "    encoding=raw_data_encoding)\n",
    "print(f\"Execution of merging took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d870d17-66f9-46f0-a101-adceb59403ee",
   "metadata": {},
   "source": [
    "### Converting TXT to CSV\n",
    "For converting the merged text file to a csv file, the delimiter \"tab\" should be replaced with semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fab704f-0bcc-48ec-9245-4d532a0cfdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(input_file_path: str, out_file_path: str = None, encoding='utf_8') -> str:\n",
    "    \"\"\"\n",
    "    Converts a single text file with tab as delimiter to a csv file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file_path (str): Full path to the text file to be converted.\n",
    "        out_file_path (str, optional): Full path to output file. Default is using the input_file_path with .csv extension (Default None).\n",
    "\n",
    "    Exceptions:\n",
    "        OSError: Raised if input_file_path does not exist or the specified out_file_path is invalid (directory does not exist)\n",
    "\n",
    "    Returns:\n",
    "        (str): Full path to the created csv file\n",
    "    \"\"\"\n",
    "    # Check input file path\n",
    "    if not os.path.isfile(input_file_path):\n",
    "        raise OSError(ERROR_MESSAGE_OSERROR.format(input_file_path))\n",
    "\n",
    "    # Check output file path or set it if no parameter was set\n",
    "    if out_file_path is None:\n",
    "        out_file_path = os.path.join(os.path.dirname(input_file_path), Path(input_file_path).stem + '.csv')\n",
    "        print(out_file_path)\n",
    "    elif not os.path.exists(os.path.dirname(out_file_path)):\n",
    "        raise OSError(f\"Path '{os.path.dirname(out_file_path)}' does not exist.\")\n",
    "\n",
    "    # Open and process text file to prepare csv\n",
    "    with open(input_file_path, 'r', encoding=encoding) as merged_input:\n",
    "        data = merged_input.readlines()\n",
    "        # Replace unicode character U+00B7 (Middle Dot) with hyphen and split by tab delimiter\n",
    "        # Results in a list of lists as base for converting to a csv file\n",
    "        data = [x.strip().replace(\"Â·\", \"-\").split(\"\\t\") for x in data]\n",
    "        # Join the inner lists with semicolon as csv delimiter        \n",
    "        data = [';'.join(x) for x in data]\n",
    "\n",
    "        # Remove rows which do not match the column count\n",
    "        data_lines = len(data)\n",
    "        print(f\"The data has {data_lines} rows.\")\n",
    "        column_count = len(data[0].split(\";\"))\n",
    "        print(f\"The data has {column_count} columns.\")\n",
    "        print(\"Skipping unsufficient rows...\")\n",
    "        data = [line for line in data if len(line.split(\";\")) == column_count]\n",
    "        print(f\"Removed {data_lines - len(data)} rows.\")\n",
    "\n",
    "        # Write to output csv\n",
    "        with open(out_file_path, 'w') as out_file:\n",
    "            out_file.write(\"\\n\".join(data))\n",
    "\n",
    "    return out_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb77571-f723-4a82-91d1-94a2a7da4e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/data_merged.csv\n",
      "The data has 138887 rows.\n",
      "The data has 20 columns.\n",
      "Skipping unsufficient rows...\n",
      "Removed 12 rows.\n",
      "Creating csv from merged txt took: 1.6483495235443115 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data_csv = txt_to_csv(raw_data_merged_file_path, encoding=raw_data_encoding)\n",
    "print(f\"Creating csv from merged txt took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c181e1ce-cc80-4be3-8bae-cd89ff1f7c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/data_merged.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ce5b37-4532-44f6-8275-abbfb1b09a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_data(input_file_path: str):\n",
    "    \"\"\"\n",
    "    Clean the header names, drop obsolete columns, apply privacy by obscuring names.\n",
    "    \n",
    "    \"\"\"\n",
    "    if not os.path.isfile(input_file_path):\n",
    "        raise OSError(ERROR_MESSAGE_OSERROR.format(input_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72c5db-fa16-41d2-8b14-fbdd1325c44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-venv",
   "language": "python",
   "name": "mlops-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
